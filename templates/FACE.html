<!DOCTYPE html>
<html>
<head>
  <title>Real Time Face Detection</title>
  <script src="/face-api.min.js"></script>

  <style>
    .video-container {
      position: relative;
      width: 720px;
      height: 560px;
    }
    
    #myCanvas {
      position: absolute;
      top: 0;
      left: 0;
    }
    
    body {
      margin: 0;
      padding: 0;
      display: flex;
      justify-content: center;
      align-items: center;
    }
  </style>
</head>
<body>
  <div class="video-container">
    <video id="myVideo" width="720" height="560" autoplay muted></video>
    <canvas id="myCanvas"></canvas>
  </div>

  <!-- ... -->

<script>
  const video = document.getElementById('myVideo');
  const canvas = document.getElementById('myCanvas');
  const context = canvas.getContext('2d');

  let labeledFaceDescriptors; 

async function loadLabeledImages() {
    const labels = ['Person 1', 'Person 2', 'Person 3'];  // replace these labels with the actual names of the people
    const labeledFaceDescriptors = await Promise.all(
        labels.map(async (label, i) => {
            const descriptions = [];
            // Assuming each person has only one image for simplicity
            const imgPath = `/IMG/${label}.jpg`;
            const img = await faceapi.fetchImage(imgPath);
            const detections = await faceapi.detectSingleFace(img).withFaceLandmarks().withFaceDescriptor();
            descriptions.push(detections.descriptor);
            
            return new faceapi.LabeledFaceDescriptors(label, descriptions);
        })
    );
    return labeledFaceDescriptors;
}


 // Will hold our training data

  Promise.all([
    faceapi.nets.tinyFaceDetector.loadFromUri('/models'),
    faceapi.nets.faceLandmark68Net.loadFromUri('/models'),
    faceapi.nets.faceRecognitionNet.loadFromUri('/models'),
    faceapi.nets.faceExpressionNet.loadFromUri('/models'),
    faceapi.nets.ageGenderNet.loadFromUri('/models')
  ]).then(async () => {
    // Load labeled images for training face recognition.
    // Replace loadLabeledImages with your function for loading and labeling images
    labeledFaceDescriptors = await loadLabeledImages();
    startVideo();
  })

  function startVideo() {
    navigator.getUserMedia(
      { video: {} },
      stream => {
        video.srcObject = stream;
        video.onloadedmetadata = () => {
          canvas.width = video.videoWidth;
          canvas.height = video.videoHeight;
        }
      },
      err => console.error(err)
    )
  }

  video.addEventListener('play', () => {
    const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors, 0.6);
    setInterval(async () => {
      const displaySize = { width: video.width, height: video.height };
      faceapi.matchDimensions(canvas, displaySize);

      const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions().withAgeAndGender();
      const resizedDetections = faceapi.resizeResults(detections, displaySize);

      context.clearRect(0, 0, canvas.width, canvas.height);
      faceapi.draw.drawDetections(canvas, resizedDetections);
      faceapi.draw.drawFaceLandmarks(canvas, resizedDetections);
      faceapi.draw.drawFaceExpressions(canvas, resizedDetections);

      resizedDetections.forEach(result => {
        const { age, gender, genderProbability } = result;

        // Display age and gender
        new faceapi.draw.DrawTextField(
          [
            `${faceapi.utils.round(age, 0)} years`,
            `${gender} (${faceapi.utils.round(genderProbability)})`
          ],
          result.detection.box.bottomRight
        ).draw(canvas);

        // Add face recognition
        const bestMatch = faceMatcher.findBestMatch(result.descriptor);
        new faceapi.draw.DrawTextField(
          [`${bestMatch.label} (${faceapi.utils.round(bestMatch.distance, 2)})`],
          result.detection.box.topRight
        ).draw(canvas);
      });
    }, 100);
  });
</script>

<!-- ... -->
</html>

